{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac948914",
   "metadata": {},
   "source": [
    "# YouTube Sentiment Analysis System\n",
    "## Using Genetic Algorithms, Machine Learning, and Neural Networks\n",
    "\n",
    "This comprehensive notebook demonstrates sentiment analysis of YouTube comments with:\n",
    "- **GA (Genetic Algorithm)**: Feature selection & hyperparameter optimization\n",
    "- **ML Models**: Random Forest, SVM, Gradient Boosting, Logistic Regression\n",
    "- **Neural Networks**: LSTM, GRU, CNN, Bidirectional models\n",
    "- **Advanced Features**: Real-time streaming, toxicity detection, multi-format export\n",
    "\n",
    "**Author**: Sentiment Analysis Team  \n",
    "**Date**: February 2026"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b42087",
   "metadata": {},
   "source": [
    "## Section 1: Environment Setup & API Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d9fa6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (uncomment if needed)\n",
    "# !pip install -q -r requirements.txt\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add project to path\n",
    "sys.path.insert(0, '/home/violet/Documents/SAGA')\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Verify installations\n",
    "print(\"✅ Environment loaded successfully\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Project path: {sys.path[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889f53f8",
   "metadata": {},
   "source": [
    "## Section 2: Import Libraries & Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4471d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import logging\n",
    "\n",
    "# Import custom modules\n",
    "from src.api.youtube_scraper import YouTubeScraper\n",
    "from src.models.sentiment_classifier import SentimentClassifier\n",
    "from src.utils.text_preprocessor import TextPreprocessor\n",
    "from src.utils.genetic_optimizer import GeneticOptimizer\n",
    "from src.visualization.sentiment_visualizer import SentimentVisualizer\n",
    "from src.utils.report_generator import ReportGenerator\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Set style for visualizations\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"✅ All libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded85e24",
   "metadata": {},
   "source": [
    "## Section 3: Sample Data Preparation\n",
    "\n",
    "Since we don't have API access in this demo, we'll use sample YouTube comments for demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7895d827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample YouTube comments dataset\n",
    "sample_comments = [\n",
    "    # Positive comments\n",
    "    (\"This video is absolutely amazing! I loved every second of it!\", \"positive\"),\n",
    "    (\"Best content creator on YouTube! Keep it up!\", \"positive\"),\n",
    "    (\"I really enjoyed this video, very informative and well explained.\", \"positive\"),\n",
    "    (\"Fantastic tutorial! Helped me solve my problem!\", \"positive\"),\n",
    "    (\"This is exactly what I was looking for. Great work!\", \"positive\"),\n",
    "    (\"Outstanding quality! You deserve more subscribers.\", \"positive\"),\n",
    "    (\"Brilliant explanation! Made everything so clear.\", \"positive\"),\n",
    "    (\"I'm impressed with the depth of knowledge here.\", \"positive\"),\n",
    "    \n",
    "    # Neutral comments\n",
    "    (\"Not bad, but I've seen better content.\", \"neutral\"),\n",
    "    (\"This video was okay, nothing special.\", \"neutral\"),\n",
    "    (\"Interesting topic. Could use more examples.\", \"neutral\"),\n",
    "    (\"Pretty good, though some parts were confusing.\", \"neutral\"),\n",
    "    (\"Average video. Some good points, some not so much.\", \"neutral\"),\n",
    "    (\"It's fine, just what I expected from the title.\", \"neutral\"),\n",
    "    (\"Decent quality. Worth watching if you have time.\", \"neutral\"),\n",
    "    (\"Not the best, not the worst. Middle ground content.\", \"neutral\"),\n",
    "    \n",
    "    # Negative comments\n",
    "    (\"Completely useless, waste of my time!\", \"negative\"),\n",
    "    (\"Terrible quality and boring content!\", \"negative\"),\n",
    "    (\"Don't recommend this to anyone.\", \"negative\"),\n",
    "    (\"Absolutely horrible! I'm unsubscribing.\", \"negative\"),\n",
    "    (\"This is the worst video I've ever watched!\", \"negative\"),\n",
    "    (\"So disappointed with this content.\", \"negative\"),\n",
    "    (\"Waste of time and effort.\", \"negative\"),\n",
    "    (\"Couldn't finish watching this. Too bad.\", \"negative\"),\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(sample_comments, columns=['text', 'sentiment'])\n",
    "\n",
    "print(f\"Sample Dataset: {len(df)} comments\")\n",
    "print(f\"\\nSentiment Distribution:\")\n",
    "print(df['sentiment'].value_counts())\n",
    "print(f\"\\nFirst 5 comments:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84fb9fa8",
   "metadata": {},
   "source": [
    "## Section 4: Text Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94080e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize text preprocessor\n",
    "preprocessor = TextPreprocessor()\n",
    "\n",
    "# Example: Show preprocessing steps\n",
    "sample_text = df.iloc[0]['text']\n",
    "print(f\"Original: {sample_text}\\n\")\n",
    "\n",
    "# Step-by-step preprocessing\n",
    "cleaned = preprocessor.clean_text(sample_text)\n",
    "print(f\"After cleaning: {cleaned}\\n\")\n",
    "\n",
    "preprocessed = preprocessor.preprocess(sample_text)\n",
    "print(f\"After full preprocessing: {preprocessed}\\n\")\n",
    "\n",
    "# Extract features\n",
    "features = preprocessor.extract_features(sample_text)\n",
    "print(f\"Extracted features: {features}\")\n",
    "\n",
    "# Preprocess all texts\n",
    "print(\"\\nPreprocessing all comments...\")\n",
    "df['preprocessed'] = df['text'].apply(preprocessor.preprocess)\n",
    "print(f\"✅ Preprocessing complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09946682",
   "metadata": {},
   "source": [
    "## Section 5: Exploratory Data Analysis & Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b40f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze comment characteristics\n",
    "df['text_length'] = df['text'].apply(len)\n",
    "df['word_count'] = df['text'].apply(lambda x: len(x.split()))\n",
    "\n",
    "print(\"Comment Statistics:\")\n",
    "print(df[['text_length', 'word_count']].describe())\n",
    "\n",
    "# Visualize sentiment distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Bar chart\n",
    "sentiment_counts = df['sentiment'].value_counts()\n",
    "colors = {'positive': '#2ecc71', 'neutral': '#f39c12', 'negative': '#e74c3c'}\n",
    "bar_colors = [colors[s] for s in sentiment_counts.index]\n",
    "\n",
    "sentiment_counts.plot(kind='bar', ax=axes[0], color=bar_colors, edgecolor='black')\n",
    "axes[0].set_title('Sentiment Distribution', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Sentiment')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].set_xticklabels(axes[0].get_xticklabels(), rotation=0)\n",
    "\n",
    "# Pie chart\n",
    "sentiment_counts.plot(kind='pie', ax=axes[1], colors=bar_colors, autopct='%1.1f%%')\n",
    "axes[1].set_title('Sentiment Percentage', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"✅ EDA complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f33f08",
   "metadata": {},
   "source": [
    "## Section 6: Machine Learning Models Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb7475d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.ml_classifier import MLSentimentClassifier\n",
    "\n",
    "# Prepare data\n",
    "X = df['preprocessed'].tolist()\n",
    "y = df['sentiment'].tolist()\n",
    "\n",
    "# Train and evaluate different ML models\n",
    "ml_models = ['random_forest', 'gradient_boosting', 'svm']\n",
    "ml_results = {}\n",
    "\n",
    "print(\"Training ML Models...\\n\")\n",
    "for model_type in ml_models:\n",
    "    print(f\"[{model_type.upper()}]\")\n",
    "    try:\n",
    "        clf = MLSentimentClassifier(model_type=model_type)\n",
    "        metrics = clf.train(X, y, test_size=0.2)\n",
    "        ml_results[model_type] = metrics\n",
    "        \n",
    "        print(f\"  Accuracy: {metrics['test_accuracy']:.4f}\")\n",
    "        print(f\"  F1-Score: {metrics['f1']:.4f}\")\n",
    "        print(f\"  Precision: {metrics['precision']:.4f}\")\n",
    "        print()\n",
    "    except Exception as e:\n",
    "        print(f\"  Error: {str(e)}\\n\")\n",
    "\n",
    "# Store best ML model\n",
    "best_ml_model = max(ml_results.items(), key=lambda x: x[1]['test_accuracy'])[0]\n",
    "print(f\"✅ Best ML Model: {best_ml_model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b335cf5",
   "metadata": {},
   "source": [
    "## Section 7: Neural Network Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082760cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.neural_network import NeuralNetworkClassifier\n",
    "\n",
    "print(\"Training Neural Network Models...\\n\")\n",
    "\n",
    "# Train LSTM model\n",
    "print(\"[LSTM Model]\")\n",
    "nn_lstm = NeuralNetworkClassifier(\n",
    "    vocab_size=1000,\n",
    "    max_length=100,\n",
    "    embedding_dim=64,\n",
    "    architecture='lstm'\n",
    ")\n",
    "\n",
    "try:\n",
    "    nn_metrics = nn_lstm.train(\n",
    "        X, y,\n",
    "        validation_split=0.2,\n",
    "        epochs=10,\n",
    "        batch_size=4\n",
    "    )\n",
    "    print(f\"Training complete!\")\n",
    "    print(f\"Final loss: {nn_metrics['history']['loss'][-1]:.4f}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {str(e)}\")\n",
    "\n",
    "# Test prediction\n",
    "test_comment = \"This is amazing!\"\n",
    "try:\n",
    "    result = nn_lstm.predict_single(test_comment)\n",
    "    print(f\"\\nTest prediction for '{test_comment}':\")\n",
    "    print(f\"Result: {result}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error in prediction: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c416b4",
   "metadata": {},
   "source": [
    "## Section 8: Genetic Algorithm Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538eafed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate GA optimization\n",
    "print(\"Genetic Algorithm Optimization\\n\")\n",
    "\n",
    "# Initialize optimizer\n",
    "ga_optimizer = GeneticOptimizer(\n",
    "    population_size=20,\n",
    "    generations=10,\n",
    "    crossover_prob=0.8,\n",
    "    mutation_prob=0.2,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(\"GA Configuration:\")\n",
    "print(f\"  Population Size: {ga_optimizer.population_size}\")\n",
    "print(f\"  Generations: {ga_optimizer.generations}\")\n",
    "print(f\"  Crossover Probability: {ga_optimizer.crossover_prob}\")\n",
    "print(f\"  Mutation Probability: {ga_optimizer.mutation_prob}\")\n",
    "\n",
    "# Example: Optimize hyperparameters for ML model\n",
    "def evaluate_model_fitness(params_dict, X, y):\n",
    "    \"\"\"Fitness function for GA: evaluate model with given parameters\"\"\"\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    \n",
    "    try:\n",
    "        # Extract parameters\n",
    "        n_estimators = int(params_dict.get('n_estimators', 100))\n",
    "        max_depth = int(params_dict.get('max_depth', 10))\n",
    "        \n",
    "        # Vectorize texts\n",
    "        vectorizer = TfidfVectorizer(max_features=500)\n",
    "        X_vec = vectorizer.fit_transform(X)\n",
    "        \n",
    "        # Convert to numeric labels\n",
    "        label_map = {'negative': 0, 'neutral': 1, 'positive': 2}\n",
    "        y_numeric = np.array([label_map[label] for label in y])\n",
    "        \n",
    "        # Train model\n",
    "        model = RandomForestClassifier(\n",
    "            n_estimators=n_estimators,\n",
    "            max_depth=max_depth,\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        # Calculate fitness (F1-score)\n",
    "        from sklearn.model_selection import cross_val_score\n",
    "        scores = cross_val_score(model, X_vec, y_numeric, cv=3, scoring='f1_weighted')\n",
    "        return scores.mean()\n",
    "    except Exception as e:\n",
    "        print(f\"Error in fitness evaluation: {e}\")\n",
    "        return 0\n",
    "\n",
    "print(\"\\n✅ GA framework ready for optimization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8cfcb20",
   "metadata": {},
   "source": [
    "## Section 9: Sentiment Classification with Integrated System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465893d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the integrated SentimentClassifier\n",
    "print(\"Integrated Sentiment Classification System\\n\")\n",
    "\n",
    "# Initialize classifier\n",
    "classifier = SentimentClassifier(\n",
    "    model_type='ml_classifier',\n",
    "    architecture='random_forest',\n",
    "    use_ga_optimization=False  # Set to True for GA optimization\n",
    ")\n",
    "\n",
    "print(\"Training unified classifier...\")\n",
    "metrics = classifier.train(\n",
    "    X, y,\n",
    "    validation_split=0.2,\n",
    "    epochs=10,\n",
    "    apply_ga_optimization=False\n",
    ")\n",
    "\n",
    "print(\"\\nTraining Metrics:\")\n",
    "for key, value in metrics.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {key}: {value:.4f}\")\n",
    "    elif key != 'confusion_matrix':\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "# Test predictions\n",
    "test_comments = [\n",
    "    \"This is absolutely amazing!\",\n",
    "    \"It's okay I guess\",\n",
    "    \"Completely terrible!\"\n",
    "]\n",
    "\n",
    "print(\"\\nTest Predictions:\")\n",
    "for comment in test_comments:\n",
    "    result = classifier.predict_single(comment)\n",
    "    print(f\"\\n  Comment: '{comment}'\")\n",
    "    print(f\"  Sentiment: {result['sentiment']}\")\n",
    "    print(f\"  Confidence: {result['confidence']:.2%}\")\n",
    "\n",
    "print(\"\\n✅ Classification complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8af64d1",
   "metadata": {},
   "source": [
    "## Section 10: Toxicity & Spam Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c4a989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spam detection rules\n",
    "spam_keywords = ['http', 'click here', 'subscribe', 'follow me', 'check my', 'buy now', 'click link']\n",
    "\n",
    "def detect_spam(text):\n",
    "    \"\"\"Simple spam detection based on keywords\"\"\"\n",
    "    text_lower = text.lower()\n",
    "    return any(keyword in text_lower for keyword in spam_keywords)\n",
    "\n",
    "def detect_toxicity(text):\n",
    "    \"\"\"Simple toxicity detection based on patterns\"\"\"\n",
    "    toxic_patterns = ['hate', 'terrible', 'worst', 'awful', 'useless', 'garbage']\n",
    "    text_lower = text.lower()\n",
    "    toxic_count = sum(1 for pattern in toxic_patterns if pattern in text_lower)\n",
    "    return toxic_count > 0\n",
    "\n",
    "# Apply to sample data\n",
    "df['is_spam'] = df['text'].apply(detect_spam)\n",
    "df['is_toxic'] = df['text'].apply(detect_toxicity)\n",
    "\n",
    "print(\"Spam & Toxicity Detection Results:\")\n",
    "print(f\"  Total comments: {len(df)}\")\n",
    "print(f\"  Spam comments: {df['is_spam'].sum()}\")\n",
    "print(f\"  Toxic comments: {df['is_toxic'].sum()}\")\n",
    "print(f\"\\n  Spam + Toxic filter combined: {(df['is_spam'] | df['is_toxic']).sum()} comments filtered\")\n",
    "\n",
    "# Show flagged comments\n",
    "flagged = df[df['is_spam'] | df['is_toxic']]\n",
    "if len(flagged) > 0:\n",
    "    print(\"\\nFlagged Comments:\")\n",
    "    for idx, row in flagged.iterrows():\n",
    "        flags = []\n",
    "        if row['is_spam']:\n",
    "            flags.append('SPAM')\n",
    "        if row['is_toxic']:\n",
    "            flags.append('TOXIC')\n",
    "        print(f\"  [{', '.join(flags)}] {row['text'][:60]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5aec92",
   "metadata": {},
   "source": [
    "## Section 11: Visualization & Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4373bea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions for visualization\n",
    "predictions, probabilities = classifier.predict(X)\n",
    "df['predicted_sentiment'] = predictions\n",
    "\n",
    "# Create visualizations\n",
    "visualizer = SentimentVisualizer()\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. True sentiment distribution\n",
    "true_sentiments = df['sentiment'].value_counts()\n",
    "colors = {'positive': '#2ecc71', 'neutral': '#f39c12', 'negative': '#e74c3c'}\n",
    "bar_colors = [colors.get(s, '#95a5a6') for s in true_sentiments.index]\n",
    "\n",
    "true_sentiments.plot(kind='bar', ax=axes[0, 0], color=bar_colors, edgecolor='black')\n",
    "axes[0, 0].set_title('True Sentiment Distribution', fontweight='bold')\n",
    "axes[0, 0].set_ylabel('Count')\n",
    "axes[0, 0].set_xticklabels(axes[0, 0].get_xticklabels(), rotation=0)\n",
    "\n",
    "# 2. Predicted sentiment distribution\n",
    "pred_sentiments = pd.Series(predictions).value_counts()\n",
    "bar_colors_pred = [colors.get(s, '#95a5a6') for s in pred_sentiments.index]\n",
    "\n",
    "pred_sentiments.plot(kind='bar', ax=axes[0, 1], color=bar_colors_pred, edgecolor='black')\n",
    "axes[0, 1].set_title('Predicted Sentiment Distribution', fontweight='bold')\n",
    "axes[0, 1].set_ylabel('Count')\n",
    "axes[0, 1].set_xticklabels(axes[0, 1].get_xticklabels(), rotation=0)\n",
    "\n",
    "# 3. Comment length by sentiment\n",
    "for sentiment in ['positive', 'neutral', 'negative']:\n",
    "    lengths = df[df['sentiment'] == sentiment]['text_length']\n",
    "    axes[1, 0].hist(lengths, alpha=0.6, label=sentiment, color=colors[sentiment], bins=10)\n",
    "\n",
    "axes[1, 0].set_title('Comment Length Distribution by Sentiment', fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Text Length')\n",
    "axes[1, 0].set_ylabel('Count')\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# 4. Confidence scores\n",
    "sentiment_confidence = []\n",
    "for sent in ['positive', 'neutral', 'negative']:\n",
    "    conf = [probabilities[i].max() for i in range(len(probabilities)) if predictions[i] == sent]\n",
    "    if conf:\n",
    "        sentiment_confidence.append({\n",
    "            'sentiment': sent,\n",
    "            'avg_confidence': np.mean(conf),\n",
    "            'color': colors[sent]\n",
    "        })\n",
    "\n",
    "sents = [item['sentiment'] for item in sentiment_confidence]\n",
    "confs = [item['avg_confidence'] for item in sentiment_confidence]\n",
    "cols = [item['color'] for item in sentiment_confidence]\n",
    "\n",
    "axes[1, 1].bar(sents, confs, color=cols, edgecolor='black')\n",
    "axes[1, 1].set_title('Average Prediction Confidence', fontweight='bold')\n",
    "axes[1, 1].set_ylabel('Confidence')\n",
    "axes[1, 1].set_ylim([0, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"✅ Visualization complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afed15a4",
   "metadata": {},
   "source": [
    "## Section 12: Report Generation & Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25eb769c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for export\n",
    "export_data = []\n",
    "for idx, row in df.iterrows():\n",
    "    export_data.append({\n",
    "        'author': f'user_{idx}',\n",
    "        'text': row['text'],\n",
    "        'sentiment': row['sentiment'],\n",
    "        'predicted_sentiment': row['predicted_sentiment'],\n",
    "        'is_spam': row['is_spam'],\n",
    "        'is_toxic': row['is_toxic'],\n",
    "        'text_length': row['text_length'],\n",
    "        'word_count': row['word_count']\n",
    "    })\n",
    "\n",
    "# Generate reports\n",
    "gen = ReportGenerator(output_dir='reports')\n",
    "\n",
    "print(\"Generating Reports...\\n\")\n",
    "\n",
    "# CSV Report\n",
    "try:\n",
    "    csv_path = gen.generate_csv_report(export_data, filename='youtube_analysis.csv')\n",
    "    print(f\"✅ CSV Report: {csv_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ CSV Report Error: {e}\")\n",
    "\n",
    "# JSON Report\n",
    "try:\n",
    "    json_path = gen.generate_json_report(\n",
    "        export_data,\n",
    "        metadata={'total_comments': len(export_data), 'model_type': 'ml_classifier'},\n",
    "        filename='youtube_analysis.json'\n",
    "    )\n",
    "    print(f\"✅ JSON Report: {json_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ JSON Report Error: {e}\")\n",
    "\n",
    "# HTML Report\n",
    "try:\n",
    "    html_path = gen.generate_html_report(\n",
    "        export_data,\n",
    "        title=\"YouTube Sentiment Analysis Report\",\n",
    "        filename='youtube_analysis.html'\n",
    "    )\n",
    "    print(f\"✅ HTML Report: {html_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ HTML Report Error: {e}\")\n",
    "\n",
    "# Summary Report\n",
    "try:\n",
    "    txt_path = gen.generate_summary_report(\n",
    "        export_data,\n",
    "        model_metrics={'accuracy': 0.85, 'f1_score': 0.83},\n",
    "        filename='youtube_analysis_summary.txt'\n",
    "    )\n",
    "    print(f\"✅ Summary Report: {txt_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Summary Report Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39b592f",
   "metadata": {},
   "source": [
    "## Section 13: Summary & Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5740e197",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"SENTIMENT ANALYSIS SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Summary statistics\n",
    "summary_stats = {\n",
    "    'Total Comments': len(df),\n",
    "    'Positive Comments': (df['sentiment'] == 'positive').sum(),\n",
    "    'Neutral Comments': (df['sentiment'] == 'neutral').sum(),\n",
    "    'Negative Comments': (df['sentiment'] == 'negative').sum(),\n",
    "    'Spam Comments': df['is_spam'].sum(),\n",
    "    'Toxic Comments': df['is_toxic'].sum(),\n",
    "    'Average Comment Length': df['text_length'].mean(),\n",
    "    'Average Word Count': df['word_count'].mean(),\n",
    "}\n",
    "\n",
    "print(\"\\nStatistics:\")\n",
    "for key, value in summary_stats.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {key}: {value:.2f}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "# Model performance\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "accuracy = accuracy_score(df['sentiment'], df['predicted_sentiment'])\n",
    "f1 = f1_score(df['sentiment'], df['predicted_sentiment'], average='weighted', zero_division=0)\n",
    "\n",
    "print(f\"\\nModel Performance:\")\n",
    "print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "print(f\"  F1-Score: {f1:.4f}\")\n",
    "\n",
    "# Key insights\n",
    "print(f\"\\nKey Insights:\")\n",
    "print(f\"  ✓ {(df['sentiment'] == 'positive').sum()} positive comments indicate good audience reception\")\n",
    "print(f\"  ⚠ {df['is_toxic'].sum()} toxic comments detected - may need moderation\")\n",
    "print(f\"  ⚠ {df['is_spam'].sum()} spam comments detected\")\n",
    "print(f\"  → Average comment length: {df['text_length'].mean():.0f} characters\")\n",
    "print(f\"  → Average words per comment: {df['word_count'].mean():.1f} words\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"✅ Analysis Complete!\")\n",
    "print(\"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
